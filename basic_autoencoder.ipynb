{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Autoencoder and Latent Space Classification Notebook\n",
    "#\n",
    "# In this notebook, we will:\n",
    "# \n",
    "# 1. **Build a Simple Autoencoder:**  \n",
    "#    Create an autoencoder (encoder and decoder) that takes 512x512 images as input.\n",
    "#\n",
    "# 2. **Data Preparation:**  \n",
    "#    Load images using PyTorch’s `ImageFolder`, apply the necessary transformations, and split the dataset into training (70%), validation (15%), and test (15%) sets.\n",
    "#\n",
    "# 3. **Training the Autoencoder:**  \n",
    "#    Train the autoencoder using Mean Squared Error (MSE) as the reconstruction loss and monitor the loss per epoch.\n",
    "#\n",
    "# 4. **Plot the Loss:**  \n",
    "#    Plot the training and validation loss per epoch and save the plot as `reconstruction_loss.png`.\n",
    "#\n",
    "# 5. **Latent Space Extraction and Classification:**  \n",
    "#    Extract the latent representations (z) from the trained autoencoder, then train a logistic regression classifier (using scikit‑learn) on these representations.\n",
    "#\n",
    "# 6. **Evaluation:**  \n",
    "#    Evaluate the classifier using the weighted F1 score.\n",
    "#\n",
    "# **Note:**  \n",
    "# Make sure to update the dataset path (`dataset_path`) to point to your dataset directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Define the Autoencoder Model\n",
    "#\n",
    "# The autoencoder is composed of:\n",
    "#\n",
    "# - **Encoder:** A series of convolutional layers that downsample the image and finally flatten the output. A fully connected layer then maps the flattened features to the latent space vector `z`.\n",
    "# - **Decoder:** A fully connected layer converts `z` back to a feature map, which is then upsampled via transposed convolution layers to reconstruct the original image.\n",
    "#\n",
    "# We assume the images have 3 channels (RGB) and are of size 512x512.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Downsample the input image.\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),  # [16, 256, 256]\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # [32, 128, 128]\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # [64, 64, 64]\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), # [128, 32, 32]\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),# [256, 16, 16]\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # Fully connected layers to go from the flattened feature map to the latent vector and back.\n",
    "        self.fc1 = nn.Linear(256 * 16 * 16, latent_dim)  # Encode to latent vector.\n",
    "        self.fc2 = nn.Linear(latent_dim, 256 * 16 * 16)   # Decode from latent vector.\n",
    "        \n",
    "        # Decoder: Upsample the feature map back to the original image dimensions.\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, \n",
    "                               padding=1, output_padding=1),  # [128, 32, 32]\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, \n",
    "                               padding=1, output_padding=1),  # [64, 64, 64]\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, \n",
    "                               padding=1, output_padding=1),  # [32, 128, 128]\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, \n",
    "                               padding=1, output_padding=1),  # [16, 256, 256]\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, \n",
    "                               padding=1, output_padding=1),  # [3, 512, 512]\n",
    "            nn.Sigmoid()  # Normalize the output to be between 0 and 1.\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Encode\n",
    "        x_enc = self.encoder(x)\n",
    "        x_enc = x_enc.view(batch_size, -1)  # Flatten the feature maps.\n",
    "        z = self.fc1(x_enc)  # Get the latent representation.\n",
    "        # Decode\n",
    "        x_dec = self.fc2(z)\n",
    "        x_dec = x_dec.view(batch_size, 256, 16, 16)  # Reshape back to feature map dimensions.\n",
    "        x_recon = self.decoder(x_dec)  # Reconstruct the image.\n",
    "        return x_recon, z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Data Loading and Splitting\n",
    "#\n",
    "# We load our dataset using `torchvision.datasets.ImageFolder`. The images are resized to 512x512 and converted to tensors.\n",
    "# The dataset is then split into:\n",
    "#\n",
    "# - **Training Set:** 70%\n",
    "# - **Validation Set:** 15%\n",
    "# - **Test Set:** 15%\n",
    "#\n",
    "# **Important:** Update the `dataset_path` variable to the location of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Data transformations: resize images to 512x512 and convert them to tensors.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Set the dataset path (update this path accordingly).\n",
    "dataset_path = 'path/to/dataset'\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Compute dataset splits.\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for each subset.\n",
    "batch_size = 8  # Adjust based on available hardware.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Training the Autoencoder\n",
    "#\n",
    "# We now train our autoencoder using MSE loss to measure the difference between the input images and their reconstructions.\n",
    "# After each epoch, we evaluate the model on the validation set and log both the training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "latent_dim = 128\n",
    "model = Autoencoder(latent_dim=latent_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "            outputs, _ = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]  Train Loss: {epoch_train_loss:.4f}  |  Val Loss: {epoch_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Plotting and Saving the Reconstruction Loss\n",
    "#\n",
    "# We plot the training and validation reconstruction loss per epoch. The plot is saved as `reconstruction_loss.png` and displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "def extract_latents(dataloader):\n",
    "    model.eval()\n",
    "    latent_list = []\n",
    "    label_list = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            _, z = model(imgs)\n",
    "            latent_list.append(z.cpu().numpy())\n",
    "            label_list.append(lbls.numpy())\n",
    "    return np.concatenate(latent_list, axis=0), np.concatenate(label_list, axis=0)\n",
    "\n",
    "train_latents, train_labels = extract_latents(train_loader)\n",
    "test_latents, test_labels = extract_latents(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Logistic Regression Classification on the Latent Space\n",
    "#\n",
    "# We now train a logistic regression classifier using the latent representations from the training set.\n",
    "# The classifier is then evaluated on the test set using the weighted F1 score as the performance metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(train_latents, train_labels)\n",
    "test_preds = clf.predict(test_latents)\n",
    "f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "print(f\"Logistic Regression Classification F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Conclusion\n",
    "#\n",
    "# In this notebook, we:\n",
    "#\n",
    "# - Built and trained an autoencoder on 512x512 images.\n",
    "# - Monitored and plotted the reconstruction loss over epochs.\n",
    "# - Extracted latent representations from the autoencoder.\n",
    "# - Performed a classification task using logistic regression on the latent space.\n",
    "# - Evaluated the classifier using the weighted F1 score.\n",
    "#\n",
    "# This pipeline can serve as a foundation for more complex models or additional downstream tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple_ae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
